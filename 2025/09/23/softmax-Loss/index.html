<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>softmax-Loss | Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="Softmax损失函数Softmax函数Softmax函数将神经网络的原始输出（未归一化的对数概率）转换为一个概率分布：$$\hat{y}j &#x3D; \frac{e^{o_j}}{\sum{k&#x3D;1}^q e^{o_k}}$$其中：  $o_j$：第j个类别的得分 $\hat{y}_j$：预测概率 $q$：类别总数  最大似然估计（MLE）的目标似然函数（假设样本独立同分布）：$$P">
<meta property="og:type" content="article">
<meta property="og:title" content="softmax-Loss">
<meta property="og:url" content="http://example.com/2025/09/23/softmax-Loss/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="Softmax损失函数Softmax函数Softmax函数将神经网络的原始输出（未归一化的对数概率）转换为一个概率分布：$$\hat{y}j &#x3D; \frac{e^{o_j}}{\sum{k&#x3D;1}^q e^{o_k}}$$其中：  $o_j$：第j个类别的得分 $\hat{y}_j$：预测概率 $q$：类别总数  最大似然估计（MLE）的目标似然函数（假设样本独立同分布）：$$P">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2025-09-23T06:37:06.000Z">
<meta property="article:modified_time" content="2025-09-23T06:41:06.735Z">
<meta property="article:author" content="John Doe">
<meta property="article:tag" content="deep-learning">
<meta property="article:tag" content="softmax">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
  
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/fork-awesome@1.2.0/css/fork-awesome.min.css">

<meta name="generator" content="Hexo 8.0.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"><span class="fa fa-bars"></span></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
        
          <a class="nav-icon" href="/atom.xml" title="RSS Feed"><span class="fa fa-rss"></span></a>
        
        <a class="nav-icon nav-search-btn" title="Search"><span class="fa fa-search"></span></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://example.com"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="post-softmax-Loss" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2025/09/23/softmax-Loss/" class="article-date">
  <time class="dt-published" datetime="2025-09-23T06:37:06.000Z" itemprop="datePublished">2025-09-23</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="p-name article-title" itemprop="headline name">
      softmax-Loss
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="Softmax损失函数"><a href="#Softmax损失函数" class="headerlink" title="Softmax损失函数"></a>Softmax损失函数</h1><h2 id="Softmax函数"><a href="#Softmax函数" class="headerlink" title="Softmax函数"></a>Softmax函数</h2><p><strong>Softmax函数</strong>将神经网络的原始输出（未归一化的对数概率）转换为一个概率分布：<br>$$<br>\hat{y}<em>j &#x3D; \frac{e^{o_j}}{\sum</em>{k&#x3D;1}^q e^{o_k}}<br>$$<br>其中：</p>
<ul>
<li>$o_j$：第j个类别的得分</li>
<li>$\hat{y}_j$：预测概率</li>
<li>$q$：类别总数</li>
</ul>
<h2 id="最大似然估计（MLE）的目标"><a href="#最大似然估计（MLE）的目标" class="headerlink" title="最大似然估计（MLE）的目标"></a>最大似然估计（MLE）的目标</h2><p><strong>似然函数</strong>（假设样本独立同分布）：<br>$$<br>P(Y \mid X) &#x3D; \prod_{i&#x3D;1}^n P(y^{(i)} \mid x^{(i)})<br>$$</p>
<p><strong>对数似然函数</strong>：<br>$$<br>\log P(Y \mid X) &#x3D; \sum_{i&#x3D;1}^n \log P(y^{(i)} \mid x^{(i)})<br>$$</p>
<h2 id="Softmax损失函数（交叉熵损失）"><a href="#Softmax损失函数（交叉熵损失）" class="headerlink" title="Softmax损失函数（交叉熵损失）"></a>Softmax损失函数（交叉熵损失）</h2><p>$$<br>L(y, \hat{y}) &#x3D; -\sum_{j&#x3D;1}^q y_j \log \hat{y}_j<br>$$</p>
<p>其中：</p>
<ul>
<li>$y_j$：真实标签的独热编码（正确类别为1，其余为0）</li>
<li>$\hat{y}_j$：Softmax输出的预测概率</li>
</ul>
<h2 id="数学推导与梯度"><a href="#数学推导与梯度" class="headerlink" title="数学推导与梯度"></a>数学推导与梯度</h2><p><strong>损失函数对未归一化得分$o_j$的梯度</strong>：<br>$$<br>\frac{\partial L}{\partial o_j} &#x3D; \hat{y}_j - y_j<br>$$</p>
<h2 id="实际应用注意事项"><a href="#实际应用注意事项" class="headerlink" title="实际应用注意事项"></a>实际应用注意事项</h2><p><strong>正则化</strong>：</p>
<ul>
<li>通常加入L2正则化项防止过拟合</li>
<li>相当于在MLE框架下引入高斯先验（最大后验估计，MAP）</li>
</ul>
<p><strong>数值稳定性</strong>：</p>
<ul>
<li>实现时需对Softmax的指数计算进行数值优化</li>
<li>常用技巧：减去最大值，避免数值溢出</li>
</ul>
<h2 id="核心联系总结"><a href="#核心联系总结" class="headerlink" title="核心联系总结"></a>核心联系总结</h2><table>
<thead>
<tr>
<th>方面</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td><strong>概率视角</strong></td>
<td>Softmax输出概率分布，MLE最大化数据似然</td>
</tr>
<tr>
<td><strong>优化视角</strong></td>
<td>交叉熵损失最小化等价于对数似然最大化</td>
</tr>
<tr>
<td><strong>梯度性质</strong></td>
<td>梯度$\hat{y}_j - y_j$直接反映概率差异</td>
</tr>
<tr>
<td><strong>正则化</strong></td>
<td>L2正则对应高斯先验，改善泛化能力</td>
</tr>
</tbody></table>
<h2 id="代码实现示例（Python）"><a href="#代码实现示例（Python）" class="headerlink" title="代码实现示例（Python）"></a>代码实现示例（Python）</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">softmax</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="comment"># 数值稳定版Softmax</span></span><br><span class="line">    x_exp = np.exp(x - np.<span class="built_in">max</span>(x))</span><br><span class="line">    <span class="keyword">return</span> x_exp / np.<span class="built_in">sum</span>(x_exp)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">cross_entropy_loss</span>(<span class="params">y_true, y_pred</span>):</span><br><span class="line">    <span class="comment"># 交叉熵损失</span></span><br><span class="line">    <span class="keyword">return</span> -np.<span class="built_in">sum</span>(y_true * np.log(y_pred + <span class="number">1e-8</span>))  <span class="comment"># 添加小值防止log(0)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 示例计算</span></span><br><span class="line">scores = np.array([<span class="number">3.0</span>, <span class="number">1.0</span>, <span class="number">0.2</span>])</span><br><span class="line">probs = softmax(scores)</span><br><span class="line">true_label = np.array([<span class="number">1.0</span>, <span class="number">0.0</span>, <span class="number">0.0</span>])  <span class="comment"># 真实标签</span></span><br><span class="line"></span><br><span class="line">loss = cross_entropy_loss(true_label, probs)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;预测概率: <span class="subst">&#123;probs&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;损失值: <span class="subst">&#123;loss&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>



<h1 id="对数似然到交叉熵的推导"><a href="#对数似然到交叉熵的推导" class="headerlink" title="对数似然到交叉熵的推导"></a>对数似然到交叉熵的推导</h1><h2 id="最大似然估计的设定"><a href="#最大似然估计的设定" class="headerlink" title="最大似然估计的设定"></a>最大似然估计的设定</h2><p>在softmax回归中，我们试图<strong>最大化</strong>观察到<strong>当前</strong>数据标签的条件概率。假设我们有数据集 ${ \mathbf{X}, \mathbf{Y} }$，其中包含 $n$ 个样本，每个样本由特征向量 $\mathbf{x}^{(i)}$ 和对应的**独热编码（one-hot encoding）**标签向量 $\mathbf{y}^{(i)}$ 组成。标签向量 $\mathbf{y}^{(i)}$ 是一个 $q$ 维向量（$q$ 是类别数量），其中只有真实类别对应的位置为 1，其余位置为 0。</p>
<p>我们的目标是找到模型参数，使得给定输入 $\mathbf{X}$ 时观察到标签 $\mathbf{Y}$ 的<strong>联合条件概率</strong>最大。假设样本是独立同分布的，这个联合概率可以表示为每个样本条件概率的连乘积：<br>$$<br>P(\mathbf{Y} \mid \mathbf{X}) &#x3D; \prod_{i&#x3D;1}^n P(\mathbf{y}^{(i)} \mid \mathbf{x}^{(i)})<br>$$</p>
<h2 id="负对数似然与损失函数"><a href="#负对数似然与损失函数" class="headerlink" title="负对数似然与损失函数"></a>负对数似然与损失函数</h2><p>在优化中，我们通常倾向于<strong>最小化</strong>一个损失函数，而不是最大化似然函数。由于对数函数是单调递增的，<strong>最大化似然函数等价于最大化对数似然函数</strong>，同时也等价于<strong>最小化负对数似然函数（Negative Log-Likelihood, NLL）</strong>。</p>
<p>因此，我们对上述联合概率取负对数：<br>$$<br>-\log P(\mathbf{Y} \mid \mathbf{X}) &#x3D; -\log \left[ \prod_{i&#x3D;1}^n P(\mathbf{y}^{(i)} \mid \mathbf{x}^{(i)}) \right] &#x3D; -\sum_{i&#x3D;1}^n \log P(\mathbf{y}^{(i)} \mid \mathbf{x}^{(i)})<br>$$</p>
<p>这里的 $-\sum_{i&#x3D;1}^n \log P(\mathbf{y}^{(i)} \mid \mathbf{x}^{(i)})$ 就是所有样本的<strong>总负对数似然损失</strong>。对于单个样本 $(i)$，其损失为 $l(\mathbf{y}^{(i)}, \hat{\mathbf{y}}^{(i)}) &#x3D; -\log P(\mathbf{y}^{(i)} \mid \mathbf{x}^{(i)})$。</p>
<h2 id="连接似然与Softmax输出概率"><a href="#连接似然与Softmax输出概率" class="headerlink" title="连接似然与Softmax输出概率"></a>连接似然与Softmax输出概率</h2><p>在softmax回归中，模型通过softmax函数将输出（logits）转换为一个概率分布。对于样本 $i$，其对于类别 $j$ 的预测概率为：<br>$$<br>\hat{y}<em>j^{(i)} &#x3D; P(y^{(i)}&#x3D;j \mid \mathbf{x}^{(i)}) &#x3D; \frac{\exp(o_j^{(i)})}{\sum</em>{k&#x3D;1}^q \exp(o_k^{(i)})}<br>$$<br>其中 $\mathbf{o}^{(i)}$ 是模型对于样本 $i$ 的原始输出（logits）。因此，整个预测概率向量 $\hat{\mathbf{y}}^{(i)}$ 可以看作是给定输入 $\mathbf{x}^{(i)}$ 时标签的条件概率分布的一个模型估计，即 $P(\mathbf{y}^{(i)} \mid \mathbf{x}^{(i)}) \approx \hat{\mathbf{y}}^{(i)}$。</p>
<h2 id="从条件概率到交叉熵形式"><a href="#从条件概率到交叉熵形式" class="headerlink" title="从条件概率到交叉熵形式"></a>从条件概率到交叉熵形式</h2><p>现在，我们来看对于单个样本 $i$ 的条件概率 $P(\mathbf{y}^{(i)} \mid \mathbf{x}^{(i)})$。由于标签 $\mathbf{y}^{(i)}$ 是独热编码向量（假设其真实类别为 $j^<em>$，即 $y_{j^</em>}^{(i)} &#x3D; 1$，其余 $y_j^{(i)} &#x3D; 0, j \neq j^<em>$），观察到这个特定标签向量的概率，实际上就是模型预测其为真实类别的概率 $\hat{y}_{j^</em>}^{(i)}$。这是因为独热编码向量中，只有真实类别位置为1，其他都为0：<br>$$<br>P(\mathbf{y}^{(i)} \mid \mathbf{x}^{(i)}) &#x3D; \prod_{j&#x3D;1}^q P(y^{(i)}&#x3D;j \mid \mathbf{x}^{(i)})^{y_j^{(i)}} &#x3D; \prod_{j&#x3D;1}^q (\hat{y}<em>j^{(i)})^{y_j^{(i)}}<br>$$<br>这个等式的含义是：因为 $y_j^{(i)}$ 只有在 $j&#x3D;j^*$（真实类别）时为1，其他都为0，所以连乘的结果就是 $\hat{y}</em>{j^*}^{(i)}$。</p>
<p>于是，对于这个样本的负对数似然为：<br>$$<br>-\log P(\mathbf{y}^{(i)} \mid \mathbf{x}^{(i)}) &#x3D; -\log \left[ \prod_{j&#x3D;1}^q (\hat{y}<em>j^{(i)})^{y_j^{(i)}} \right] &#x3D; -\sum</em>{j&#x3D;1}^q y_j^{(i)} \log \hat{y}_j^{(i)}<br>$$<br>最后这一步运用了对数的性质 $\log(a^b) &#x3D; b \log a$ 以及连乘的对数等于对数的求和。</p>
<p>我们注意到，$-\sum_{j&#x3D;1}^q y_j^{(i)} \log \hat{y}_j^{(i)}$ 正是<strong>交叉熵损失函数（Cross-Entropy Loss）</strong> 的标准定义。它衡量了真实标签的分布 $\mathbf{y}^{(i)}$（这里是独热编码）与模型预测的分布 $\hat{\mathbf{y}}^{(i)}$ 之间的差异。</p>
<!---more--->

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2025/09/23/softmax-Loss/" data-id="cuids4WmSDM8xBq3l1kZHPNzp" data-title="softmax-Loss" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/deep-learning/" rel="tag">deep-learning</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/softmax/" rel="tag">softmax</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
  
    <a href="/2025/09/22/github%E5%BB%BA%E7%AB%99%E6%95%99%E7%A8%8B/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">github建站教程</div>
    </a>
  
</nav>

  
</article>


</section>
        
          <aside id="sidebar">
  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/deep-learning/" rel="tag">deep-learning</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/softmax/" rel="tag">softmax</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/deep-learning/" style="font-size: 10px;">deep-learning</a> <a href="/tags/softmax/" style="font-size: 10px;">softmax</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2025/09/">September 2025</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2025/09/23/softmax-Loss/">softmax-Loss</a>
          </li>
        
          <li>
            <a href="/2025/09/22/github%E5%BB%BA%E7%AB%99%E6%95%99%E7%A8%8B/">github建站教程</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2025 John Doe<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.6.4.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>